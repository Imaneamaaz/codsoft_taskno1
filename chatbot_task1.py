# -*- coding: utf-8 -*-
"""Chatbot_Task1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13DBMmW1T-5smhzGDA4iwY1IisUeBoUca

# Part1
"""

pip install nltk

!pip install torch torchvision torchaudio

import nltk

import torch
print(torch.__version__)

print(torch.cuda.is_available())

import nltk
from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()

import numpy as np
nltk.download('punkt')

def tokenize(sentence):
    return nltk.word_tokenize(sentence)

def stem(word):
    return stemmer.stem(word.lower())

def bag_of_words(tokenized_sentence, words):
    # Stem each word
    sentence_words = [stem(word) for word in tokenized_sentence]
    # Initialize bag with 0 for each word
    bag = np.zeros(len(words), dtype=np.float32)
    for idx, w in enumerate(words):
        if w in sentence_words:
            bag[idx] = 1
    return bag

"""# Part2

"""

import json

with open('/bin/intents.json','r') as f:
  intents = json.load(f)
print(intents)

all_words = []
tags = []
data_pairs = []
for intent in intents['intents']:
    tag = intent['tag']
    tags.append(tag)
    for pattern in intent['patterns']:
        # Tokenize each word in the sentence
        tokens = tokenize(pattern)
        # Add to our words list
        all_words.extend(tokens)
        # Add to data pairs
        data_pairs.append((tokens, tag))

# Stem and lower each word
ignore_words = ['?', '.', '!']
all_words = [stem(w) for w in all_words if w not in ignore_words]
# Remove duplicates and sort
all_words = sorted(set(all_words))
tags = sorted(set(tags))

print(len(data_pairs), "patterns")
print(len(tags), "tags:", tags)
print(len(all_words), "unique stemmed words:", all_words)

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
# Create training data
X_train = []
y_train = []
for (pattern_sentence, tag) in data_pairs:
    # X: bag of words for each pattern_sentence
    bag = bag_of_words(pattern_sentence, all_words)
    X_train.append(bag)
    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot
    label = tags.index(tag)
    y_train.append(label)
X_train = np.array(X_train)
y_train = np.array(y_train)

# Hyper-parameters
num_epochs = 1000
batch_size = 8
learning_rate = 0.001
input_size = len(X_train[0])
hidden_size = 8
output_size = len(tags)

class ChatDataset(Dataset):
    def __init__(self):
        self.n_samples = len(X_train)
        self.x_data = X_train
        self.y_data = y_train

    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    def __len__(self):
        return self.n_samples

dataset = ChatDataset()
train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=0)

"""# Pytorch model and training

"""

import torch
import torch.nn as nn

# Define model
class ChatModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_labels):
        super(ChatModel, self).__init__()
        self.layer1 = nn.Linear(input_dim, hidden_dim)
        self.layer2 = nn.Linear(hidden_dim, hidden_dim)
        self.layer3 = nn.Linear(hidden_dim, num_labels)
        self.activation = nn.ReLU()

    def forward(self, x):
        x = self.layer1(x)
        x = self.activation(x)
        x = self.layer2(x)
        x = self.activation(x)
        x = self.layer3(x)
        return x

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ChatModel(input_dim=input_size, hidden_dim=hidden_size, num_labels=output_size).to(device)

# Loss and optimizer
loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Train the model
for epoch in range(num_epochs):
    for (batch_words, batch_labels) in train_loader:
        batch_words = batch_words.to(device)
        batch_labels = batch_labels.to(dtype=torch.long).to(device)

        # Forward pass
        predictions = model(batch_words)
        loss = loss_function(predictions, batch_labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if (epoch + 1) % 100 == 0:
            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')

print(f'Final loss: {loss.item():.4f}')

"""# Save and load the model"""

# Save model data
model_data = {
    "model_params": model.state_dict(),
    "input_dim": input_size,
    "hidden_dim": hidden_size,
    "num_labels": output_size,
    "word_list": all_words,
    "label_tags": tags
}

SAVE_PATH = "model_data.pth"
torch.save(model_data, SAVE_PATH)
print(f'Training complete. File saved to {SAVE_PATH}')

import random
import json

import torch

# Load the model
import random
import json

model_data = torch.load(SAVE_PATH)

input_dim = model_data["input_dim"]
hidden_dim = model_data["hidden_dim"]
num_labels = model_data["num_labels"]
word_list = model_data['word_list']
label_tags = model_data['label_tags']
model_params = model_data["model_params"]

model = ChatModel(input_dim, hidden_dim, num_labels).to(device)
model.load_state_dict(model_params)
model.eval()

bot_name = "Imane"
print("Hi there! (type 'quit' to exit)")
while True:
    user_input = input("User: ")
    if user_input.lower() == "quit":
        break

    tokens = tokenize(user_input)
    input_vector = bag_of_words(tokens, word_list)
    input_vector = input_vector.reshape(1, input_vector.shape[0])
    input_vector = torch.from_numpy(input_vector).to(device)

    output = model(input_vector)
    _, predicted_label = torch.max(output, dim=1)

    tag = label_tags[predicted_label.item()]

    probabilities = torch.softmax(output, dim=1)
    probability = probabilities[0][predicted_label.item()]
    if probability.item() > 0.75:
        for intent in intents['intents']:
            if tag == intent["tag"]:
                print(f"{bot_name}: {random.choice(intent['responses'])}")
    else:
        print(f"{bot_name}: I did not get what you mean, can you reformulate?")

